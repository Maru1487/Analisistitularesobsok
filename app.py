# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13WYdqID4EOLhb4sFOCTldQ30YGU_-n7o
"""

import pandas as pd
from google.colab import files
from IPython.display import display

def cargar_y_procesar_reportes():
    print("üìÇ Sub√≠ los dos archivos CSV descargados desde Marfeel:")
    archivos_subidos = files.upload()

    if len(archivos_subidos) != 2:
        raise ValueError("Deb√©s subir exactamente dos archivos.")

    archivos = list(archivos_subidos.keys())

    # Cargar CSVs
    df1 = pd.read_csv(archivos[0])
    df2 = pd.read_csv(archivos[1])

    # Normalizar columnas
    def normalizar_columnas(df):
        df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")
        return df

    df1 = normalizar_columnas(df1)
    df2 = normalizar_columnas(df2)

    # Detectar cu√°l es el de fuentes y cu√°l el de totales
    if 'sourceinternal' in df1.columns:
        df_fuentes = df1
        df_totales = df2
    else:
        df_fuentes = df2
        df_totales = df1

    # Agrupar fuentes por t√≠tulo
    df_fuentes_agg = df_fuentes.groupby(['title', 'sourceinternal'])['pageviewstotal'].sum().reset_index()

    # Determinar fuente principal
    idx_max = df_fuentes_agg.groupby('title')['pageviewstotal'].idxmax()
    df_fuente_principal = df_fuentes_agg.loc[idx_max].reset_index(drop=True)
    df_fuente_principal = df_fuente_principal.rename(columns={
        'pageviewstotal': 'pageviews_fuente_principal',
        'sourceinternal': 'fuente_principal'
    })

    # Unir con totales
    df_final = pd.merge(df_totales, df_fuente_principal, on='title', how='left')

    # Calcular porcentaje de fuente principal
    df_final['porcentaje_fuente_principal'] = (
        df_final['pageviews_fuente_principal'] / df_final['pageviewstotal'] * 100
    ).round(2)

    # Resultado final ordenado
    resultado = df_final[['title', 'pageviewstotal', 'fuente_principal', 'porcentaje_fuente_principal']]
    resultado = resultado.sort_values(by='pageviewstotal', ascending=False).reset_index(drop=True)

    return resultado

# Ejecutar procesamiento
resultado = cargar_y_procesar_reportes()

# Mostrar top 20
display(resultado.head(20))

# Funci√≥n auxiliar: filtro tem√°tico por palabra clave
def filtrar_por_tema(df, palabra_clave):
    palabra = palabra_clave.lower()
    filtrado = df[df['title'].str.lower().str.contains(palabra, na=False)].copy()
    return filtrado.reset_index(drop=True)

filtrar_por_tema(resultado, "Pe√±arol")

!pip install -U spacy
!python -m spacy download es_core_news_sm

import spacy
nlp = spacy.load('es_core_news_sm')

# An√°lisis 1: Longitud del t√≠tulo (en caracteres)
resultado['longitud_titulo'] = resultado['title'].apply(len)

# Vista previa con la nueva variable
display(resultado[['title', 'longitud_titulo', 'pageviewstotal', 'fuente_principal']].head(10))

import re

# Funci√≥n combinada de extracci√≥n de entidades
def extraer_entidades(titulo):
    doc = nlp(titulo)

    # Entidades detectadas autom√°ticamente por spaCy
    entidades_spacy = [ent.text for ent in doc.ents if ent.label_ in ("PER", "ORG", "LOC")]

    # Detecci√≥n manual de entidades locales por coincidencia exacta como palabra (case insensitive)
    entidades_locales_encontradas = [
        ent for ent in entidades_locales_uy
        if re.search(rf'\b{re.escape(ent)}\b', titulo, flags=re.IGNORECASE)
    ]

    # Combinar y eliminar duplicados
    entidades_total = list(set(entidades_spacy + entidades_locales_encontradas))
    return entidades_total

# Aplicar al DataFrame
resultado["entidades"] = resultado["title"].apply(extraer_entidades)

# Variable booleana: ¬øel t√≠tulo tiene entidades?
resultado["tiene_entidades"] = resultado["entidades"].apply(lambda x: len(x) > 0)

# Contador: ¬øcu√°ntas entidades tiene el t√≠tulo?
resultado["cantidad_entidades"] = resultado["entidades"].apply(len)

# Vista previa
display(resultado[["title", "entidades", "tiene_entidades", "cantidad_entidades", "pageviewstotal"]].head(10))

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  An√°lisis 3 ‚Äì Tono del t√≠tulo (interrogativo / exclamativo)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Palabras / frases que suelen introducir preguntas impl√≠citas
palabras_interrogativas = [
    # b√°sicas
    "qu√©", "qui√©n", "qui√©nes", "cu√°ndo", "c√≥mo", "d√≥nde", "cu√°l", "cu√°les",
    "por qu√©", "para qu√©",
    # ampliadas (uso local)
    "qu√© pas√≥ con", "qu√© dice", "qu√© opinan", "qu√© se sabe",
    "qui√©n es", "cu√°nto cuesta", "hasta cu√°ndo",
    "c√≥mo funciona", "por qu√© ahora", "va a cambiar", "es verdad que"
]

# Expresiones que marcan sorpresa / √©nfasis (sin signos)
expresiones_exclamativas = [
    # b√°sicas
    "ins√≥lito", "sorprendente", "impresionante", "incre√≠ble",
    "no lo vas a creer", "dram√°tico", "emotivo", "desgarrador", "esc√°ndalo",
    # ampliadas (uso local)
    "atenci√≥n", "alerta", "de pel√≠cula", "de no creer", "pol√©mico",
    "conmoci√≥n", "indignaci√≥n", "impactante", "grave denuncia",
    "inesperado", "hist√≥rico", "se viraliz√≥", "fuertes im√°genes",
    "reacci√≥n en redes", "revelaci√≥n", "testimonio desgarrador", "rompi√≥ el silencio",
    "estall√≥"
]

def clasificar_tono(titulo: str) -> str:
    t_low = titulo.lower()

    # 1. Signos expl√≠citos
    if any(s in titulo for s in ["?", "¬ø"]):
        return "interrogativo"
    if any(s in titulo for s in ["!", "¬°"]):
        return "exclamativo"

    # 2. Patrones impl√≠citos
    if any(frase in t_low for frase in palabras_interrogativas):
        return "interrogativo"
    if any(frase in t_low for frase in expresiones_exclamativas):
        return "exclamativo"

    return "neutro"

# ‚îÄ Aplicar al DataFrame (ya existente) ‚îÄ
resultado["tono_titulo"] = resultado["title"].apply(clasificar_tono)

# Vista r√°pida
from IPython.display import display
display(
    resultado[["title", "tono_titulo", "pageviewstotal", "fuente_principal"]]
    .head(30)
)

import re

def detectar_cita(titulo: str) -> str:
    # Busca contenido entre comillas dobles (tipogr√°ficas o rectas)
    return "S√≠" if re.search(r'["‚Äú‚Äù](.+?)["‚Äú‚Äù]', titulo) else "No"

# Aplicar al DataFrame
resultado["tiene_cita"] = resultado["title"].apply(detectar_cita)

# Vista r√°pida
display(resultado[["title", "tiene_cita", "pageviewstotal", "fuente_principal"]].head(15))

# Funci√≥n para clasificar la posici√≥n de la primera entidad encontrada en el t√≠tulo
def posicion_entidad(titulo, entidades):
    if not entidades:
        return "sin_entidades"

    titulo_lower = titulo.lower()
    primera_entidad = None
    pos_entidad = len(titulo)  # valor alto por defecto

    for ent in entidades:
        idx = titulo_lower.find(ent.lower())
        if idx != -1 and idx < pos_entidad:
            pos_entidad = idx
            primera_entidad = ent

    if primera_entidad is None:
        return "sin_entidades"

    longitud = len(titulo)
    proporcion = pos_entidad / longitud

    if proporcion <= 0.25:
        return "inicio"
    elif proporcion >= 0.75:
        return "final"
    else:
        return "medio"

# Aplicar al DataFrame
resultado["posicion_entidad"] = resultado.apply(
    lambda row: posicion_entidad(row["title"], row["entidades"]),
    axis=1
)

# Vista previa
resultado[["title", "entidades", "posicion_entidad", "pageviewstotal"]].head(10)

# Funci√≥n para detectar el estilo narrativo o informativo
def detectar_estilo_titulo(titulo):
    doc = nlp(titulo)
    verbos = [token for token in doc if token.pos_ == "VERB"]

    # Si hay verbos conjugados en pasado o presente, probablemente sea narrativo
    tiempos_narrativos = {"Pres", "Past", "Imp", "Ind"}
    verbos_conjugados = [
        v for v in verbos if v.morph.get("Tense") and any(t in v.morph.get("Tense") for t in tiempos_narrativos)
    ]

    # Si hay muchos sustantivos pero ning√∫n verbo conjugado ‚Üí probablemente informativo
    sustantivos = [token for token in doc if token.pos_ == "NOUN"]

    if len(verbos_conjugados) > 0:
        return "narrativo"
    elif len(sustantivos) > 3 and len(verbos_conjugados) == 0:
        return "informativo"
    else:
        return "informativo"

# Aplicar al DataFrame
resultado["estilo_titulo"] = resultado["title"].apply(detectar_estilo_titulo)

# Vista previa
resultado[["title", "estilo_titulo", "pageviewstotal"]].head(10)

# Lista confirmada de verbos declarativos
verbos_declarativos = [
    "anunci√≥", "anunciar√°", "confirm√≥", "confirmar√°", "decidi√≥", "decidir√°",
    "rechaz√≥", "rechazar√°", "defendi√≥", "defender√°", "cuestion√≥", "cuestionar√°",
    "propuso", "propondr√°", "sostuvo", "sostendr√°", "denunci√≥", "denunciar√°",
    "convoc√≥", "convocar√°", "present√≥", "presentar√°", "pidi√≥", "pedir√°",
    "apoy√≥", "apoyar√°", "critic√≥", "criticar√°n", "celebr√≥", "celebrar√°"
]

# Funci√≥n para clasificar el estilo del t√≠tulo
def clasificar_estilo_titulo(titulo, entidades, verbos_declarativos):
    doc = nlp(titulo)

    tokens = [token.text.lower() for token in doc]
    tokens_verbos = [token for token in doc if token.pos_ == "VERB"]
    tokens_nombres = [token for token in doc if token.pos_ in ("PROPN", "NOUN")]

    # 1. Declarativo: contiene verbo declarativo + al menos una entidad
    if any(verbo in tokens for verbo in verbos_declarativos) and entidades:
        return "declarativo"

    # 2. Narrativo: contiene verbo + sujeto o nombre propio
    if tokens_verbos and tokens_nombres:
        return "narrativo"

    # 3. Por descarte
    return "informativo-descriptivo"

# Aplicar la funci√≥n
resultado["estilo_titulo"] = resultado.apply(
    lambda row: clasificar_estilo_titulo(row["title"], row["entidades"], verbos_declarativos),
    axis=1
)

# Vista previa
display(resultado[["title", "estilo_titulo", "entidades", "pageviewstotal"]].head(10))